<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Midterm_summaries | RNN Class</title>
<meta name="keywords" content="">
<meta name="description" content="Midterm_summaries - RNN Class">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/midterm_summary/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.41696966e8df279036a27475a76b05b7e8c7617949cab3fec7496401d66c1c47.css" integrity="sha256-QWlpZujfJ5A2onR1p2sFt&#43;jHYXlJyrP&#43;x0lkAdZsHEc=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/midterm_summary/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/midterm_summary/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="RNN Class (Alt + H)">RNN Class</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/midterm_summary/chapter-1/" title="Chapter 1">
                    <span>Chapter 1</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/midterm_summary/chapter-2/" title="Chapter 2">
                    <span>Chapter 2</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/midterm_summary/chapter-3/" title="Chapter 3">
                    <span>Chapter 3</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/midterm_summary/chapter-4/" title="Chapter 4">
                    <span>Chapter 4</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    Midterm_summaries
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Chapter 2
    </h2>
  </header>
  <div class="entry-content">
    <p> Midterm Exam Summary: Probabilistic Graphical Models (PGMs) 1. Introduction to Probabilistic Graphical Models (PGMs) PGMs provide a declarative representation of our understanding of the world. Key Benefits: Can handle uncertainty (partial/noisy observations, missing data). Separate knowledge from reasoning—avoiding special-purpose algorithms. Allow for different inference algorithms on the same model. Types of PGMs: Directed Graphs (Bayesian Networks) Undirected Graphs (Markov Random Fields) 2. Bayesian Networks (BNs) Definition: A Bayesian Network is a Directed Acyclic Graph (DAG) that provides a compact factorized representation of a joint probability distribution. Components: Nodes: Represent random variables. Edges: Represent causal influences. Factorization follows the chain rule: P(X_1, X_2, ..., X_n) = \prod_{i} P(X_i | Parents(X_i)) Bayesian Networks - Example A student’s grade depends on intelligence, course difficulty, SAT score, and recommendation letter. Example of a Conditional Probability Table (CPT): P(X_n | Z_n = k) = P(X_n; \mu_k, \Sigma_k) Inference in Bayesian Networks Exact Inference:
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-10 13:12:43 -0400 EDT'>March 10, 2025</span></footer>
  <a class="entry-link" aria-label="post link to Chapter 2" href="http://localhost:1313/midterm_summary/chapter-2/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Chapter 3
    </h2>
  </header>
  <div class="entry-content">
    <p> Chapter 3: Deep Belief Network (DBN) 1. Introduction to Deep Belief Networks (DBN) Deep Belief Networks (DBNs) are generative models composed of multiple layers of hidden units. They are constructed using Restricted Boltzmann Machines (RBMs), which are stacked to form a deep structure. RBMs serve as the building blocks of DBNs and are a special type of Boltzmann Machines (BMs), which belong to the family of Markov Random Fields (MRFs). Boltzmann Machines (BMs) A Boltzmann Machine is a stochastic neural network consisting of symmetrically connected binary units. It is used to learn probability distributions over data. Restricted Boltzmann Machines (RBMs) RBMs are a special case of Boltzmann Machines with a restricted architecture: No intra-layer connections (i.e., no connections between neurons in the same layer). Composed of: Visible units: $ v \in {0,1}^D $ Hidden units: $ h \in {0,1}^D $ Why Use DBNs? DBNs solve the difficulty in training deep neural networks by: Pretraining each layer independently as an RBM. Using a greedy layer-wise training approach to initialize deep networks efficiently. Improving a lower bound on the likelihood with each additional layer. 2. Restricted Boltzmann Machine (RBM) Structure of an RBM RBMs consist of: A visible layer containing observed data. A hidden layer capturing latent features. A weight matrix ( W ) connecting visible and hidden units. Bias terms ( a ) and ( b ) for visible and hidden layers, respectively. Energy Function of an RBM The energy function for an RBM is defined as:
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-10 13:12:43 -0400 EDT'>March 10, 2025</span></footer>
  <a class="entry-link" aria-label="post link to Chapter 3" href="http://localhost:1313/midterm_summary/chapter-3/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Chapter 4
    </h2>
  </header>
  <div class="entry-content">
    <p> Midterm Exam Summary Neural Networks Overview Neural networks are universal approximators that can model Boolean, categorical, or real-valued functions. Capable of pattern recognition in static inputs, time-series, and sequences. Training is required for prediction. Neural networks consist of multiple layers: Input Layer: Accepts raw data. Hidden Layers: Extracts features through weighted connections. Output Layer: Produces final prediction or classification. Common activation functions: Sigmoid: $ \sigma(x) = \frac{1}{1 &#43; e^{-x}} $ (good for probabilities) ReLU: $ f(x) = \max(0, x) $ (reduces vanishing gradient problem) Tanh: $ \tanh(x) = \frac{e^x - e^{-x}}{e^x &#43; e^{-x}} $ (zero-centered outputs) 1-D Non-Linearly Separable Data Consider a one-dimensional dataset where class labels cannot be separated by a simple threshold. Estimating probability: The probability of $Y=1$ at a given point is computed using a small surrounding window. The goal is to find a function that can model $P(Y=1 | X)$. Estimating the Model A sigmoid perceptron with a single input can approximate the posterior probability of a class given input: $$ P(Y=1 | X) = \sigma(WX &#43; b) $$ Training is performed using gradient descent to optimize parameters $W$ and $b$. Maximum Likelihood Estimation (MLE) MLE minimizes the KL divergence between the desired and actual output. The loss function is derived from likelihood principles: $$ L(\theta) = -\sum_{i} y_i \log P(Y_i | X_i, \theta) &#43; (1 - y_i) \log(1 - P(Y_i | X_i, \theta)) $$ Requires gradient descent for optimization. Network Structure The final neuron in the network acts as a linear classifier on the transformed data. The network transforms non-linear data into linearly separable feature space. Feature extractor layers convert input space $X$ into feature space $Y$ where classes are separable. Feature extraction layers include convolutional layers (CNNs), recurrent layers (RNNs), and transformer layers. Autoencoder (AE) Definition: A neural network that copies its input to its output. Encoder: $h = f(x)$ Decoder: $r = g(h)$ Used for dimensionality reduction and feature extraction. Applications: Image compression Feature extraction for classification Denoising corrupted signals Linear vs. Non-Linear Autoencoders Linear AE minimizes squared reconstruction error (like PCA). Non-Linear AE finds a non-linear manifold that best represents the data. Loss function for reconstruction: $$ L(x, g(f(x))) = || x - g(f(x)) ||^2 $$ Deep Autoencoder Multi-layer Autoencoders capture more complex structures. Trained using Restricted Boltzmann Machines (RBMs) followed by fine-tuning with backpropagation. Layer-wise pretraining is often used to initialize deep autoencoders before fine-tuning. Avoiding Trivial Identity Mapping Undercomplete Autoencoders Restrict $h$ to lower dimension than $x$. Loss function: $$ L(x, g(f(x))) $$ Non-linear encoders extract useful features beyond PCA. Overcomplete Autoencoders Higher dimensional $h$ may lead to trivial identity mapping. Regularization techniques: Sparse AE: Encourages sparsity. Denoising AE: Recovers corrupted input. Contractive AE: Encourages robustness to perturbations. Variational Autoencoder (VAE) Imposes a constraint on the latent variable $z$. Ensures $z$ follows a Gaussian distribution: $$ p(z) = \mathcal{N}(0, I) $$ Uses reparameterization trick to enable backpropagation: $$ z = \mu &#43; \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) $$ VAE Loss Function Combination of reconstruction loss and KL divergence loss: $$ L = \mathbb{E}[\log p(x|z)] - D_{KL}(q(z|x) || p(z)) $$ KL divergence ensures that the latent variable follows a Gaussian distribution: $$ D_{KL}(q(z|x) || p(z)) = \frac{1}{2} \sum \left( \sigma^2 &#43; \mu^2 - 1 - \log \sigma^2 \right) $$ Gaussian Mixture Models (GMM) Used for modeling complex distributions as a combination of Gaussian distributions. Objective: Learn means, variances, and mixing probabilities. EM Algorithm is commonly used for training GMMs. Generative Models Neural networks can be used as generative models to learn probability distributions. Examples: Variational Autoencoders (VAEs) Generative Adversarial Networks (GANs) Diffusion Models Applications include image generation, text synthesis, and semantic hashing. Applications of Autoencoders Dimensionality reduction Feature learning for classification Image retrieval using binary codes (semantic hashing) Data denoising Anomaly detection in cybersecurity and healthcare Conclusion Autoencoders provide powerful tools for representation learning. VAEs extend AE capabilities by enabling probabilistic data generation. Regularization techniques ensure meaningful feature extraction. Generative models play a crucial role in modern AI applications. </p>
  </div>
  <footer class="entry-footer"><span title='2025-03-10 13:12:43 -0400 EDT'>March 10, 2025</span></footer>
  <a class="entry-link" aria-label="post link to Chapter 4" href="http://localhost:1313/midterm_summary/chapter-4/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Chapter 1
    </h2>
  </header>
  <div class="entry-content">
    <p> Neural Network Fundamentals Multi-layered Neural Network Structure: Input: 10 numbers Output: 4 decisions or predictions Backpropagation: Used to calculate the error in predictions and adjust the network. Sensitivity in Neural Networks Sensitivity measures how much the error at the output ((\Delta E)) changes due to small input variations. Sensitivity is propagated backward using: $$ \delta_{p1} = \frac{\partial E}{\partial p1} $$ Error computation block: \{2(o1 - t1), 2(o2 - t2), 2(o3 - t3), 2(o4 - t4)\} Activation Functions Sigmoid Function \sigma(x) = \frac{1}{1 &#43; e^{-x}} Outputs range: (0,1) Issues: Saturates and kills gradients. Not zero-centered. Computationally expensive. Tanh Function \tanh(x) = \frac{e^x - e^{-x}}{e^x &#43; e^{-x}} Outputs range: (-1,1) Zero-centered but still prone to gradient saturation. ReLU (Rectified Linear Unit) f(x) = \max(0, x) Does not saturate in the positive region. Computationally efficient. Drawbacks: Can lead to “dead neurons” (never activating). Not zero-centered. Leaky ReLU f(x) = \max(\alpha x, x), \quad \alpha \approx 0.01 Addresses dead neuron issue by allowing small negative values. ELU (Exponential Linear Unit) f(x) = \begin{cases} x, &amp; x &gt; 0 \\ \alpha (e^x - 1), &amp; x \leq 0 \end{cases} Zero-centered. More robust than ReLU in noise-prone settings. Maxout f(x) = \max(w_1^T x &#43; b_1, w_2^T x &#43; b_2) Generalizes ReLU and Leaky ReLU. Doubles parameters per neuron. Practical Choice Use ReLU as the default. Try Leaky ReLU, Maxout, or ELU. Avoid Sigmoid. Data Preprocessing Zero Mean X \leftarrow X - \text{mean}(X, \text{axis}=0) Normalization X \leftarrow \frac{X}{\text{std}(X, \text{axis}=0)} PCA &amp; Whitening PCA decorrelates data. Whitening scales dimensions by eigenvalues. Key Rule Compute statistics only on training data. Apply transformation to test/validation data. Weight Initialization Proper initialization prevents gradient vanishing or explosion. w = \frac{\text{randn}(n)}{\sqrt{n}} For ReLU: w = \frac{\text{randn}(n)}{\sqrt{n/2}} Batch Normalization (BN) Reduces internal covariate shift. BN normalizes activations: \hat{x} = \frac{x - \mu}{\sigma} Applied before the activation function. At test time, uses fixed mean/std from training. Regularization L2 Regularization (Weight Decay) R(f) = \lambda ||w||^2 L1 Regularization R(f) = \lambda ||w|| Leads to sparse weights. Elastic Net Combination of L1 and L2. Dropout Randomly drops neurons during training. Test-time: Scale weights by dropout probability ( p ). Hyperparameter Optimization Random Search &gt; Grid Search. Bayesian Optimization speeds up the process. Common Hyperparameters: Learning rate. Batch size. Number of layers. Dropout rate. Loss Functions Classification Softmax with Cross-Entropy Loss: L = - \sum_{i} y_i \log(\hat{y}_i) Hinge Loss (for SVMs): L = \sum_{i} \max(0, 1 - y_i \hat{y}_i) Regression L2 Loss: L = \sum_i (y_i - \hat{y}_i)^2 L1 Loss: L = \sum_i |y_i - \hat{y}_i| Parameter Update After computing gradients via backpropagation, parameters are updated using: Vanilla SGD w \leftarrow w - \eta \nabla w Momentum Update v \leftarrow \beta v - \eta \nabla w w \leftarrow w &#43; v Nesterov Accelerated Gradient v \leftarrow \beta v - \eta \nabla w&#39; w&#39; \leftarrow w &#43; \beta v Final Notes Choose activation functions wisely (ReLU recommended). Use normalization and proper weight initialization. Apply regularization techniques (L2, Dropout). Optimize hyperparameters systematically (Bayesian search preferred). Understand loss functions based on classification vs. regression. </p>
  </div>
  <footer class="entry-footer"><span title='2025-03-10 12:39:55 -0400 EDT'>March 10, 2025</span></footer>
  <a class="entry-link" aria-label="post link to Chapter 1" href="http://localhost:1313/midterm_summary/chapter-1/"></a>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">RNN Class</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
