<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Chapter 3 | RNN Class</title>
<meta name="keywords" content="">
<meta name="description" content="












    
    Chapter 3: Deep Belief Network (DBN)
1. Introduction to Deep Belief Networks (DBN)

Deep Belief Networks (DBNs) are generative models composed of multiple layers of hidden units.
They are constructed using Restricted Boltzmann Machines (RBMs), which are stacked to form a deep structure.
RBMs serve as the building blocks of DBNs and are a special type of Boltzmann Machines (BMs), which belong to the family of Markov Random Fields (MRFs).

Boltzmann Machines (BMs)

A Boltzmann Machine is a stochastic neural network consisting of symmetrically connected binary units.
It is used to learn probability distributions over data.

Restricted Boltzmann Machines (RBMs)

RBMs are a special case of Boltzmann Machines with a restricted architecture:

No intra-layer connections (i.e., no connections between neurons in the same layer).
Composed of:

Visible units: $ v \in {0,1}^D $
Hidden units: $ h \in {0,1}^D $





Why Use DBNs?

DBNs solve the difficulty in training deep neural networks by:

Pretraining each layer independently as an RBM.
Using a greedy layer-wise training approach to initialize deep networks efficiently.
Improving a lower bound on the likelihood with each additional layer.




2. Restricted Boltzmann Machine (RBM)
Structure of an RBM

RBMs consist of:

A visible layer containing observed data.
A hidden layer capturing latent features.
A weight matrix ( W ) connecting visible and hidden units.
Bias terms ( a ) and ( b ) for visible and hidden layers, respectively.



Energy Function of an RBM
The energy function for an RBM is defined as:">
<meta name="author" content="">
<link rel="canonical" href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-3/">
<link crossorigin="anonymous" href="/rnn_class/assets/css/stylesheet.41696966e8df279036a27475a76b05b7e8c7617949cab3fec7496401d66c1c47.css" integrity="sha256-QWlpZujfJ5A2onR1p2sFt&#43;jHYXlJyrP&#43;x0lkAdZsHEc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://felipecordero.github.io/rnn_class/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://felipecordero.github.io/rnn_class/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://felipecordero.github.io/rnn_class/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://felipecordero.github.io/rnn_class/apple-touch-icon.png">
<link rel="mask-icon" href="https://felipecordero.github.io/rnn_class/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-3/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-3/">
  <meta property="og:site_name" content="RNN Class">
  <meta property="og:title" content="Chapter 3">
  <meta property="og:description" content=" Chapter 3: Deep Belief Network (DBN) 1. Introduction to Deep Belief Networks (DBN) Deep Belief Networks (DBNs) are generative models composed of multiple layers of hidden units. They are constructed using Restricted Boltzmann Machines (RBMs), which are stacked to form a deep structure. RBMs serve as the building blocks of DBNs and are a special type of Boltzmann Machines (BMs), which belong to the family of Markov Random Fields (MRFs). Boltzmann Machines (BMs) A Boltzmann Machine is a stochastic neural network consisting of symmetrically connected binary units. It is used to learn probability distributions over data. Restricted Boltzmann Machines (RBMs) RBMs are a special case of Boltzmann Machines with a restricted architecture: No intra-layer connections (i.e., no connections between neurons in the same layer). Composed of: Visible units: $ v \in {0,1}^D $ Hidden units: $ h \in {0,1}^D $ Why Use DBNs? DBNs solve the difficulty in training deep neural networks by: Pretraining each layer independently as an RBM. Using a greedy layer-wise training approach to initialize deep networks efficiently. Improving a lower bound on the likelihood with each additional layer. 2. Restricted Boltzmann Machine (RBM) Structure of an RBM RBMs consist of: A visible layer containing observed data. A hidden layer capturing latent features. A weight matrix ( W ) connecting visible and hidden units. Bias terms ( a ) and ( b ) for visible and hidden layers, respectively. Energy Function of an RBM The energy function for an RBM is defined as:">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="midterm_summary">
    <meta property="article:published_time" content="2025-03-10T13:12:43-04:00">
    <meta property="article:modified_time" content="2025-03-10T13:12:43-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3">
<meta name="twitter:description" content="












    
    Chapter 3: Deep Belief Network (DBN)
1. Introduction to Deep Belief Networks (DBN)

Deep Belief Networks (DBNs) are generative models composed of multiple layers of hidden units.
They are constructed using Restricted Boltzmann Machines (RBMs), which are stacked to form a deep structure.
RBMs serve as the building blocks of DBNs and are a special type of Boltzmann Machines (BMs), which belong to the family of Markov Random Fields (MRFs).

Boltzmann Machines (BMs)

A Boltzmann Machine is a stochastic neural network consisting of symmetrically connected binary units.
It is used to learn probability distributions over data.

Restricted Boltzmann Machines (RBMs)

RBMs are a special case of Boltzmann Machines with a restricted architecture:

No intra-layer connections (i.e., no connections between neurons in the same layer).
Composed of:

Visible units: $ v \in {0,1}^D $
Hidden units: $ h \in {0,1}^D $





Why Use DBNs?

DBNs solve the difficulty in training deep neural networks by:

Pretraining each layer independently as an RBM.
Using a greedy layer-wise training approach to initialize deep networks efficiently.
Improving a lower bound on the likelihood with each additional layer.




2. Restricted Boltzmann Machine (RBM)
Structure of an RBM

RBMs consist of:

A visible layer containing observed data.
A hidden layer capturing latent features.
A weight matrix ( W ) connecting visible and hidden units.
Bias terms ( a ) and ( b ) for visible and hidden layers, respectively.



Energy Function of an RBM
The energy function for an RBM is defined as:">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Midterm_summaries",
      "item": "https://felipecordero.github.io/rnn_class/midterm_summary/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Chapter 3",
      "item": "https://felipecordero.github.io/rnn_class/midterm_summary/chapter-3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Chapter 3",
  "name": "Chapter 3",
  "description": " Chapter 3: Deep Belief Network (DBN) 1. Introduction to Deep Belief Networks (DBN) Deep Belief Networks (DBNs) are generative models composed of multiple layers of hidden units. They are constructed using Restricted Boltzmann Machines (RBMs), which are stacked to form a deep structure. RBMs serve as the building blocks of DBNs and are a special type of Boltzmann Machines (BMs), which belong to the family of Markov Random Fields (MRFs). Boltzmann Machines (BMs) A Boltzmann Machine is a stochastic neural network consisting of symmetrically connected binary units. It is used to learn probability distributions over data. Restricted Boltzmann Machines (RBMs) RBMs are a special case of Boltzmann Machines with a restricted architecture: No intra-layer connections (i.e., no connections between neurons in the same layer). Composed of: Visible units: $ v \\in {0,1}^D $ Hidden units: $ h \\in {0,1}^D $ Why Use DBNs? DBNs solve the difficulty in training deep neural networks by: Pretraining each layer independently as an RBM. Using a greedy layer-wise training approach to initialize deep networks efficiently. Improving a lower bound on the likelihood with each additional layer. 2. Restricted Boltzmann Machine (RBM) Structure of an RBM RBMs consist of: A visible layer containing observed data. A hidden layer capturing latent features. A weight matrix ( W ) connecting visible and hidden units. Bias terms ( a ) and ( b ) for visible and hidden layers, respectively. Energy Function of an RBM The energy function for an RBM is defined as:\n",
  "keywords": [
    
  ],
  "articleBody": " Chapter 3: Deep Belief Network (DBN) 1. Introduction to Deep Belief Networks (DBN) Deep Belief Networks (DBNs) are generative models composed of multiple layers of hidden units. They are constructed using Restricted Boltzmann Machines (RBMs), which are stacked to form a deep structure. RBMs serve as the building blocks of DBNs and are a special type of Boltzmann Machines (BMs), which belong to the family of Markov Random Fields (MRFs). Boltzmann Machines (BMs) A Boltzmann Machine is a stochastic neural network consisting of symmetrically connected binary units. It is used to learn probability distributions over data. Restricted Boltzmann Machines (RBMs) RBMs are a special case of Boltzmann Machines with a restricted architecture: No intra-layer connections (i.e., no connections between neurons in the same layer). Composed of: Visible units: $ v \\in {0,1}^D $ Hidden units: $ h \\in {0,1}^D $ Why Use DBNs? DBNs solve the difficulty in training deep neural networks by: Pretraining each layer independently as an RBM. Using a greedy layer-wise training approach to initialize deep networks efficiently. Improving a lower bound on the likelihood with each additional layer. 2. Restricted Boltzmann Machine (RBM) Structure of an RBM RBMs consist of: A visible layer containing observed data. A hidden layer capturing latent features. A weight matrix ( W ) connecting visible and hidden units. Bias terms ( a ) and ( b ) for visible and hidden layers, respectively. Energy Function of an RBM The energy function for an RBM is defined as:\n$$ E(v, h) = - v^T W h - a^T v - b^T h $$\nwhere:\n( W ) is the weight matrix. ( a ) is the bias vector for visible units. ( b ) is the bias vector for hidden units. Probability Distributions in RBMs The joint probability distribution is given by: $$ P(v, h) = \\frac{1}{Z} \\exp(-E(v, h)) $$\nwhere ( Z ) is the partition function:\n$$ Z = \\sum_{v,h} \\exp(-E(v, h)) $$\nThe marginal probability of visible units: $$ P(v) = \\sum_h P(v, h) $$\n3. Inference in DBNs Exact Inference Exact inference is generally intractable because: The latent space is high-dimensional. The posterior distribution is complex and difficult to compute directly. Approximated Inference (Sampling) To estimate expectations efficiently, sampling methods are used: Basic Sampling - Transformation Method Generates samples using a transformation function. Basic Sampling - Rejection Sampling Uses a proposal distribution to generate samples and rejects unlikely ones. Gibbs Sampling A Markov Chain Monte Carlo (MCMC) method where each variable is sampled conditionally. Gibbs Sampling for RBMs Used to approximate ( P(v) ).\nEach step involves:\nUpdating one variable ( z_i ) while keeping others fixed: $$ p(z_i | z_{\\setminus i}) $$\nExample Process:\nStart with an initial state ( (v_0, h_0) ). Iteratively update the state using conditional probabilities. 4. Learning in RBMs Maximum Likelihood Learning Learning is done by maximizing the likelihood function: $$ \\max_{w_{ij}, a_i, b_j} \\sum_{l=1}^{m} \\log \\sum_{h} P(v^l, h^l) $$\nThe gradient of the log-likelihood: $$ \\frac{\\partial \\log P(v)}{\\partial W_{ij}} = \\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{model}} $$\nContrastive Divergence (CD) Algorithm: A sampling-based approximation for computing the gradient efficiently. 5. Deep Belief Networks (DBN) Greedy Layer-Wise Training DBNs use a layer-wise training approach: Train the first RBM using input data. Use the activations from the trained RBM as input to the next RBM. Repeat the process for each subsequent layer. DBN Feature Generation A DBN can extract hierarchical features from data. Example: Architecture: 784-1000-500-250-30 Each layer learns a higher-level representation. DBN Applications Feature Learning Dimensionality Reduction Image Reconstruction Classification Tasks (e.g., fingerprint recognition) 6. Classification Using DBNs DBNs can be used for image classification and biometric recognition. Example: Fingerprint liveness detection. Determines whether a fingerprint is real or fake before recognition. Advantages of DBNs in Classification Efficient training using pretraining Captures deep feature representations Generalizes well across different datasets Summary of Key Equations Concept Equation Energy Function $$ E(v, h) = - v^T W h - a^T v - b^T h $$ Joint Probability $$ P(v, h) = \\frac{1}{Z} \\exp(-E(v, h)) $$ Partition Function $$ Z = \\sum_{v,h} \\exp(-E(v, h)) $$ Marginal Probability $$ P(v) = \\sum_h P(v, h) $$ Gradient Update for Learning $$ \\frac{\\partial \\log P(v)}{\\partial W_{ij}} = \\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{model}} $$ Conclusion This chapter covers Deep Belief Networks (DBNs) and their foundation in Restricted Boltzmann Machines (RBMs). The key takeaways are:\nRBMs serve as the building blocks of DBNs. DBNs use a layer-wise training approach. Learning in RBMs is done using Maximum Likelihood and Contrastive Divergence. DBNs are widely used in feature extraction, dimensionality reduction, and classification. This markdown file now contains expanded details, complete equations, and formatted LaTeX math expressions. Save it as dbn_summary.md and open it in a markdown viewer with LaTeX support for a well-formatted document. ðŸš€\n",
  "wordCount" : "801",
  "inLanguage": "en",
  "datePublished": "2025-03-10T13:12:43-04:00",
  "dateModified": "2025-03-10T13:12:43-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://felipecordero.github.io/rnn_class/midterm_summary/chapter-3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "RNN Class",
    "logo": {
      "@type": "ImageObject",
      "url": "https://felipecordero.github.io/rnn_class/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://felipecordero.github.io/rnn_class/" accesskey="h" title="RNN Class (Alt + H)">RNN Class</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-1/" title="Chapter 1">
                    <span>Chapter 1</span>
                </a>
            </li>
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-2/" title="Chapter 2">
                    <span>Chapter 2</span>
                </a>
            </li>
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-3/" title="Chapter 3">
                    <span class="active">Chapter 3</span>
                </a>
            </li>
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-4/" title="Chapter 4">
                    <span>Chapter 4</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Chapter 3
    </h1>
    <div class="post-meta"><span title='2025-03-10 13:12:43 -0400 EDT'>March 10, 2025</span>

</div>
  </header> 
  <div class="post-content"><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    document.querySelectorAll("pre code.language-math").forEach((block) => {
        const mathContent = block.textContent.trim();
        const displayMode = mathContent.includes("\n") ? "\\[" + mathContent + "\\]" : "\\(" + mathContent + "\\)";
        const wrapper = document.createElement("div");
        wrapper.innerHTML = displayMode;
        block.parentNode.replaceWith(wrapper);
    });

    if (window.MathJax) {
        MathJax.typesetPromise();
    }
});
</script>




    
    <h1 id="chapter-3-deep-belief-network-dbn">Chapter 3: Deep Belief Network (DBN)<a hidden class="anchor" aria-hidden="true" href="#chapter-3-deep-belief-network-dbn">#</a></h1>
<h2 id="1-introduction-to-deep-belief-networks-dbn">1. Introduction to Deep Belief Networks (DBN)<a hidden class="anchor" aria-hidden="true" href="#1-introduction-to-deep-belief-networks-dbn">#</a></h2>
<ul>
<li><strong>Deep Belief Networks (DBNs)</strong> are generative models composed of <strong>multiple layers of hidden units</strong>.</li>
<li>They are constructed using <strong>Restricted Boltzmann Machines (RBMs)</strong>, which are stacked to form a deep structure.</li>
<li><strong>RBMs</strong> serve as the building blocks of DBNs and are a special type of <strong>Boltzmann Machines (BMs)</strong>, which belong to the family of <strong>Markov Random Fields (MRFs)</strong>.</li>
</ul>
<h3 id="boltzmann-machines-bms"><strong>Boltzmann Machines (BMs)</strong><a hidden class="anchor" aria-hidden="true" href="#boltzmann-machines-bms">#</a></h3>
<ul>
<li>A <strong>Boltzmann Machine</strong> is a <strong>stochastic neural network</strong> consisting of symmetrically connected binary units.</li>
<li>It is used to <strong>learn probability distributions</strong> over data.</li>
</ul>
<h3 id="restricted-boltzmann-machines-rbms"><strong>Restricted Boltzmann Machines (RBMs)</strong><a hidden class="anchor" aria-hidden="true" href="#restricted-boltzmann-machines-rbms">#</a></h3>
<ul>
<li>RBMs are a <strong>special case of Boltzmann Machines</strong> with a restricted architecture:
<ul>
<li><strong>No intra-layer connections</strong> (i.e., no connections between neurons in the same layer).</li>
<li>Composed of:
<ul>
<li><strong>Visible units</strong>: $ v \in {0,1}^D $</li>
<li><strong>Hidden units</strong>: $ h \in {0,1}^D $</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="why-use-dbns"><strong>Why Use DBNs?</strong><a hidden class="anchor" aria-hidden="true" href="#why-use-dbns">#</a></h3>
<ul>
<li>DBNs solve the <strong>difficulty in training deep neural networks</strong> by:
<ol>
<li><strong>Pretraining</strong> each layer independently as an RBM.</li>
<li><strong>Using a greedy layer-wise training approach</strong> to initialize deep networks efficiently.</li>
<li><strong>Improving a lower bound on the likelihood</strong> with each additional layer.</li>
</ol>
</li>
</ul>
<hr>
<h2 id="2-restricted-boltzmann-machine-rbm">2. Restricted Boltzmann Machine (RBM)<a hidden class="anchor" aria-hidden="true" href="#2-restricted-boltzmann-machine-rbm">#</a></h2>
<h3 id="structure-of-an-rbm"><strong>Structure of an RBM</strong><a hidden class="anchor" aria-hidden="true" href="#structure-of-an-rbm">#</a></h3>
<ul>
<li>RBMs consist of:
<ul>
<li>A <strong>visible layer</strong> containing observed data.</li>
<li>A <strong>hidden layer</strong> capturing latent features.</li>
<li>A <strong>weight matrix</strong> ( W ) connecting visible and hidden units.</li>
<li><strong>Bias terms</strong> ( a ) and ( b ) for visible and hidden layers, respectively.</li>
</ul>
</li>
</ul>
<h3 id="energy-function-of-an-rbm"><strong>Energy Function of an RBM</strong><a hidden class="anchor" aria-hidden="true" href="#energy-function-of-an-rbm">#</a></h3>
<p>The <strong>energy function</strong> for an RBM is defined as:</p>
<p>$$
E(v, h) = - v^T W h - a^T v - b^T h
$$</p>
<p>where:</p>
<ul>
<li>( W ) is the weight matrix.</li>
<li>( a ) is the bias vector for visible units.</li>
<li>( b ) is the bias vector for hidden units.</li>
</ul>
<h3 id="probability-distributions-in-rbms"><strong>Probability Distributions in RBMs</strong><a hidden class="anchor" aria-hidden="true" href="#probability-distributions-in-rbms">#</a></h3>
<ul>
<li>The <strong>joint probability distribution</strong> is given by:</li>
</ul>
<p>$$
P(v, h) = \frac{1}{Z} \exp(-E(v, h))
$$</p>
<p>where ( Z ) is the <strong>partition function</strong>:</p>
<p>$$
Z = \sum_{v,h} \exp(-E(v, h))
$$</p>
<ul>
<li>The <strong>marginal probability of visible units</strong>:</li>
</ul>
<p>$$
P(v) = \sum_h P(v, h)
$$</p>
<hr>
<h2 id="3-inference-in-dbns">3. Inference in DBNs<a hidden class="anchor" aria-hidden="true" href="#3-inference-in-dbns">#</a></h2>
<h3 id="exact-inference"><strong>Exact Inference</strong><a hidden class="anchor" aria-hidden="true" href="#exact-inference">#</a></h3>
<ul>
<li>Exact inference is generally <strong>intractable</strong> because:
<ul>
<li>The <strong>latent space is high-dimensional</strong>.</li>
<li>The <strong>posterior distribution is complex</strong> and difficult to compute directly.</li>
</ul>
</li>
</ul>
<h3 id="approximated-inference-sampling"><strong>Approximated Inference (Sampling)</strong><a hidden class="anchor" aria-hidden="true" href="#approximated-inference-sampling">#</a></h3>
<ul>
<li>To estimate expectations efficiently, <strong>sampling methods</strong> are used:
<ol>
<li><strong>Basic Sampling - Transformation Method</strong>
<ul>
<li>Generates samples using a transformation function.</li>
</ul>
</li>
<li><strong>Basic Sampling - Rejection Sampling</strong>
<ul>
<li>Uses a proposal distribution to generate samples and rejects unlikely ones.</li>
</ul>
</li>
<li><strong>Gibbs Sampling</strong>
<ul>
<li>A <strong>Markov Chain Monte Carlo (MCMC)</strong> method where each variable is sampled conditionally.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="gibbs-sampling-for-rbms"><strong>Gibbs Sampling for RBMs</strong><a hidden class="anchor" aria-hidden="true" href="#gibbs-sampling-for-rbms">#</a></h4>
<ul>
<li>
<p>Used to approximate ( P(v) ).</p>
</li>
<li>
<p>Each step involves:</p>
<ul>
<li>Updating one variable ( z_i ) while keeping others fixed:</li>
</ul>
<p>$$
p(z_i | z_{\setminus i})
$$</p>
</li>
<li>
<p><strong>Example Process</strong>:</p>
<ul>
<li>Start with an initial state ( (v_0, h_0) ).</li>
<li>Iteratively update the state using conditional probabilities.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-learning-in-rbms">4. Learning in RBMs<a hidden class="anchor" aria-hidden="true" href="#4-learning-in-rbms">#</a></h2>
<h3 id="maximum-likelihood-learning"><strong>Maximum Likelihood Learning</strong><a hidden class="anchor" aria-hidden="true" href="#maximum-likelihood-learning">#</a></h3>
<ul>
<li>Learning is done by maximizing the <strong>likelihood function</strong>:</li>
</ul>
<p>$$
\max_{w_{ij}, a_i, b_j} \sum_{l=1}^{m} \log \sum_{h} P(v^l, h^l)
$$</p>
<ul>
<li>The gradient of the <strong>log-likelihood</strong>:</li>
</ul>
<p>$$
\frac{\partial \log P(v)}{\partial W_{ij}} = \langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{model}}
$$</p>
<ul>
<li><strong>Contrastive Divergence (CD) Algorithm</strong>:
<ul>
<li>A <strong>sampling-based approximation</strong> for computing the gradient efficiently.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="5-deep-belief-networks-dbn">5. Deep Belief Networks (DBN)<a hidden class="anchor" aria-hidden="true" href="#5-deep-belief-networks-dbn">#</a></h2>
<h3 id="greedy-layer-wise-training"><strong>Greedy Layer-Wise Training</strong><a hidden class="anchor" aria-hidden="true" href="#greedy-layer-wise-training">#</a></h3>
<ul>
<li>DBNs use a <strong>layer-wise</strong> training approach:
<ol>
<li>Train the <strong>first RBM</strong> using input data.</li>
<li>Use the activations from the trained RBM as input to the <strong>next RBM</strong>.</li>
<li>Repeat the process for <strong>each subsequent layer</strong>.</li>
</ol>
</li>
</ul>
<h3 id="dbn-feature-generation"><strong>DBN Feature Generation</strong><a hidden class="anchor" aria-hidden="true" href="#dbn-feature-generation">#</a></h3>
<ul>
<li>A DBN can <strong>extract hierarchical features</strong> from data.</li>
<li>Example:
<ul>
<li><strong>Architecture</strong>: 784-1000-500-250-30</li>
<li>Each layer learns a <strong>higher-level representation</strong>.</li>
</ul>
</li>
</ul>
<h3 id="dbn-applications"><strong>DBN Applications</strong><a hidden class="anchor" aria-hidden="true" href="#dbn-applications">#</a></h3>
<ul>
<li><strong>Feature Learning</strong></li>
<li><strong>Dimensionality Reduction</strong></li>
<li><strong>Image Reconstruction</strong></li>
<li><strong>Classification Tasks</strong> (e.g., fingerprint recognition)</li>
</ul>
<hr>
<h2 id="6-classification-using-dbns">6. Classification Using DBNs<a hidden class="anchor" aria-hidden="true" href="#6-classification-using-dbns">#</a></h2>
<ul>
<li>DBNs can be used for <strong>image classification</strong> and <strong>biometric recognition</strong>.</li>
<li><strong>Example</strong>: Fingerprint <strong>liveness detection</strong>.
<ul>
<li>Determines whether a fingerprint is <strong>real or fake</strong> before recognition.</li>
</ul>
</li>
</ul>
<h3 id="advantages-of-dbns-in-classification"><strong>Advantages of DBNs in Classification</strong><a hidden class="anchor" aria-hidden="true" href="#advantages-of-dbns-in-classification">#</a></h3>
<ul>
<li><strong>Efficient training using pretraining</strong></li>
<li><strong>Captures deep feature representations</strong></li>
<li><strong>Generalizes well across different datasets</strong></li>
</ul>
<hr>
<h2 id="summary-of-key-equations">Summary of Key Equations<a hidden class="anchor" aria-hidden="true" href="#summary-of-key-equations">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Concept</th>
          <th>Equation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Energy Function</strong></td>
          <td>$$ E(v, h) = - v^T W h - a^T v - b^T h $$</td>
      </tr>
      <tr>
          <td><strong>Joint Probability</strong></td>
          <td>$$ P(v, h) = \frac{1}{Z} \exp(-E(v, h)) $$</td>
      </tr>
      <tr>
          <td><strong>Partition Function</strong></td>
          <td>$$ Z = \sum_{v,h} \exp(-E(v, h)) $$</td>
      </tr>
      <tr>
          <td><strong>Marginal Probability</strong></td>
          <td>$$ P(v) = \sum_h P(v, h) $$</td>
      </tr>
      <tr>
          <td><strong>Gradient Update for Learning</strong></td>
          <td>$$ \frac{\partial \log P(v)}{\partial W_{ij}} = \langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{model}} $$</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>This chapter covers <strong>Deep Belief Networks (DBNs)</strong> and their foundation in <strong>Restricted Boltzmann Machines (RBMs)</strong>. The key takeaways are:</p>
<ul>
<li><strong>RBMs serve as the building blocks of DBNs</strong>.</li>
<li><strong>DBNs use a layer-wise training approach</strong>.</li>
<li><strong>Learning in RBMs is done using Maximum Likelihood and Contrastive Divergence</strong>.</li>
<li><strong>DBNs are widely used in feature extraction, dimensionality reduction, and classification</strong>.</li>
</ul>
<hr>
<p>This markdown file now contains <strong>expanded details, complete equations, and formatted LaTeX math expressions</strong>. Save it as <code>dbn_summary.md</code> and open it in a markdown viewer with LaTeX support for a well-formatted document. ðŸš€</p>




  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://felipecordero.github.io/rnn_class/">RNN Class</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
