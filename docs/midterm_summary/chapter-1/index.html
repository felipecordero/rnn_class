<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Chapter 1 | RNN Class</title>
<meta name="keywords" content="">
<meta name="description" content="









Neural Network Fundamentals
Multi-layered Neural Network

Structure:

Input: 10 numbers
Output: 4 decisions or predictions


Backpropagation:

Used to calculate the error in predictions and adjust the network.



Sensitivity in Neural Networks

Sensitivity measures how much the error at the output ((\Delta E)) changes due to small input variations.
Sensitivity is propagated backward using:
$$
\delta_{p1} = \frac{\partial E}{\partial p1}
$$
Error computation block:
\{2(o1 - t1), 2(o2 - t2), 2(o3 - t3), 2(o4 - t4)\}


Activation Functions
Sigmoid Function
\sigma(x) = \frac{1}{1 &#43; e^{-x}}

Outputs range: (0,1)
Issues:

Saturates and kills gradients.
Not zero-centered.
Computationally expensive.



Tanh Function
\tanh(x) = \frac{e^x - e^{-x}}{e^x &#43; e^{-x}}

Outputs range: (-1,1)
Zero-centered but still prone to gradient saturation.

ReLU (Rectified Linear Unit)
f(x) = \max(0, x)

Does not saturate in the positive region.
Computationally efficient.
Drawbacks:

Can lead to &ldquo;dead neurons&rdquo; (never activating).
Not zero-centered.



Leaky ReLU
f(x) = \max(\alpha x, x), \quad \alpha \approx 0.01

Addresses dead neuron issue by allowing small negative values.

ELU (Exponential Linear Unit)
f(x) = \begin{cases}
x, &amp; x &gt; 0 \\
\alpha (e^x - 1), &amp; x \leq 0
\end{cases}

Zero-centered.
More robust than ReLU in noise-prone settings.

Maxout
f(x) = \max(w_1^T x &#43; b_1, w_2^T x &#43; b_2)

Generalizes ReLU and Leaky ReLU.
Doubles parameters per neuron.

Practical Choice

Use ReLU as the default.
Try Leaky ReLU, Maxout, or ELU.
Avoid Sigmoid.

Data Preprocessing
Zero Mean
X \leftarrow X - \text{mean}(X, \text{axis}=0)
Normalization
X \leftarrow \frac{X}{\text{std}(X, \text{axis}=0)}
PCA &amp; Whitening

PCA decorrelates data.
Whitening scales dimensions by eigenvalues.

Key Rule

Compute statistics only on training data.
Apply transformation to test/validation data.

Weight Initialization

Proper initialization prevents gradient vanishing or explosion.

w = \frac{\text{randn}(n)}{\sqrt{n}}

For ReLU:

w = \frac{\text{randn}(n)}{\sqrt{n/2}}
Batch Normalization (BN)

Reduces internal covariate shift.
BN normalizes activations:

\hat{x} = \frac{x - \mu}{\sigma}

Applied before the activation function.
At test time, uses fixed mean/std from training.

Regularization
L2 Regularization (Weight Decay)
R(f) = \lambda ||w||^2
L1 Regularization
R(f) = \lambda ||w||

Leads to sparse weights.

Elastic Net

Combination of L1 and L2.

Dropout

Randomly drops neurons during training.
Test-time: Scale weights by dropout probability ( p ).

Hyperparameter Optimization

Random Search &gt; Grid Search.
Bayesian Optimization speeds up the process.
Common Hyperparameters:

Learning rate.
Batch size.
Number of layers.
Dropout rate.



Loss Functions
Classification

Softmax with Cross-Entropy Loss:

L = - \sum_{i} y_i \log(\hat{y}_i)

Hinge Loss (for SVMs):

L = \sum_{i} \max(0, 1 - y_i \hat{y}_i)
Regression

L2 Loss:

L = \sum_i (y_i - \hat{y}_i)^2

L1 Loss:

L = \sum_i |y_i - \hat{y}_i|
Parameter Update

After computing gradients via backpropagation, parameters are updated using:

Vanilla SGD
w \leftarrow w - \eta \nabla w
Momentum Update
v \leftarrow \beta v - \eta \nabla w
w \leftarrow w &#43; v
Nesterov Accelerated Gradient
v \leftarrow \beta v - \eta \nabla w&#39;
w&#39; \leftarrow w &#43; \beta v
Final Notes

Choose activation functions wisely (ReLU recommended).
Use normalization and proper weight initialization.
Apply regularization techniques (L2, Dropout).
Optimize hyperparameters systematically (Bayesian search preferred).
Understand loss functions based on classification vs. regression.
">
<meta name="author" content="">
<link rel="canonical" href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-1/">
<link crossorigin="anonymous" href="/rnn_class/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://felipecordero.github.io/rnn_class/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://felipecordero.github.io/rnn_class/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://felipecordero.github.io/rnn_class/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://felipecordero.github.io/rnn_class/apple-touch-icon.png">
<link rel="mask-icon" href="https://felipecordero.github.io/rnn_class/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-1/">
  <meta property="og:site_name" content="RNN Class">
  <meta property="og:title" content="Chapter 1">
  <meta property="og:description" content=" Neural Network Fundamentals Multi-layered Neural Network Structure: Input: 10 numbers Output: 4 decisions or predictions Backpropagation: Used to calculate the error in predictions and adjust the network. Sensitivity in Neural Networks Sensitivity measures how much the error at the output ((\Delta E)) changes due to small input variations. Sensitivity is propagated backward using: $$ \delta_{p1} = \frac{\partial E}{\partial p1} $$ Error computation block: \{2(o1 - t1), 2(o2 - t2), 2(o3 - t3), 2(o4 - t4)\} Activation Functions Sigmoid Function \sigma(x) = \frac{1}{1 &#43; e^{-x}} Outputs range: (0,1) Issues: Saturates and kills gradients. Not zero-centered. Computationally expensive. Tanh Function \tanh(x) = \frac{e^x - e^{-x}}{e^x &#43; e^{-x}} Outputs range: (-1,1) Zero-centered but still prone to gradient saturation. ReLU (Rectified Linear Unit) f(x) = \max(0, x) Does not saturate in the positive region. Computationally efficient. Drawbacks: Can lead to “dead neurons” (never activating). Not zero-centered. Leaky ReLU f(x) = \max(\alpha x, x), \quad \alpha \approx 0.01 Addresses dead neuron issue by allowing small negative values. ELU (Exponential Linear Unit) f(x) = \begin{cases} x, &amp; x &gt; 0 \\ \alpha (e^x - 1), &amp; x \leq 0 \end{cases} Zero-centered. More robust than ReLU in noise-prone settings. Maxout f(x) = \max(w_1^T x &#43; b_1, w_2^T x &#43; b_2) Generalizes ReLU and Leaky ReLU. Doubles parameters per neuron. Practical Choice Use ReLU as the default. Try Leaky ReLU, Maxout, or ELU. Avoid Sigmoid. Data Preprocessing Zero Mean X \leftarrow X - \text{mean}(X, \text{axis}=0) Normalization X \leftarrow \frac{X}{\text{std}(X, \text{axis}=0)} PCA &amp; Whitening PCA decorrelates data. Whitening scales dimensions by eigenvalues. Key Rule Compute statistics only on training data. Apply transformation to test/validation data. Weight Initialization Proper initialization prevents gradient vanishing or explosion. w = \frac{\text{randn}(n)}{\sqrt{n}} For ReLU: w = \frac{\text{randn}(n)}{\sqrt{n/2}} Batch Normalization (BN) Reduces internal covariate shift. BN normalizes activations: \hat{x} = \frac{x - \mu}{\sigma} Applied before the activation function. At test time, uses fixed mean/std from training. Regularization L2 Regularization (Weight Decay) R(f) = \lambda ||w||^2 L1 Regularization R(f) = \lambda ||w|| Leads to sparse weights. Elastic Net Combination of L1 and L2. Dropout Randomly drops neurons during training. Test-time: Scale weights by dropout probability ( p ). Hyperparameter Optimization Random Search &gt; Grid Search. Bayesian Optimization speeds up the process. Common Hyperparameters: Learning rate. Batch size. Number of layers. Dropout rate. Loss Functions Classification Softmax with Cross-Entropy Loss: L = - \sum_{i} y_i \log(\hat{y}_i) Hinge Loss (for SVMs): L = \sum_{i} \max(0, 1 - y_i \hat{y}_i) Regression L2 Loss: L = \sum_i (y_i - \hat{y}_i)^2 L1 Loss: L = \sum_i |y_i - \hat{y}_i| Parameter Update After computing gradients via backpropagation, parameters are updated using: Vanilla SGD w \leftarrow w - \eta \nabla w Momentum Update v \leftarrow \beta v - \eta \nabla w w \leftarrow w &#43; v Nesterov Accelerated Gradient v \leftarrow \beta v - \eta \nabla w&#39; w&#39; \leftarrow w &#43; \beta v Final Notes Choose activation functions wisely (ReLU recommended). Use normalization and proper weight initialization. Apply regularization techniques (L2, Dropout). Optimize hyperparameters systematically (Bayesian search preferred). Understand loss functions based on classification vs. regression. ">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="midterm_summary">
    <meta property="article:published_time" content="2025-03-10T12:39:55-04:00">
    <meta property="article:modified_time" content="2025-03-10T12:39:55-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1">
<meta name="twitter:description" content="









Neural Network Fundamentals
Multi-layered Neural Network

Structure:

Input: 10 numbers
Output: 4 decisions or predictions


Backpropagation:

Used to calculate the error in predictions and adjust the network.



Sensitivity in Neural Networks

Sensitivity measures how much the error at the output ((\Delta E)) changes due to small input variations.
Sensitivity is propagated backward using:
$$
\delta_{p1} = \frac{\partial E}{\partial p1}
$$
Error computation block:
\{2(o1 - t1), 2(o2 - t2), 2(o3 - t3), 2(o4 - t4)\}


Activation Functions
Sigmoid Function
\sigma(x) = \frac{1}{1 &#43; e^{-x}}

Outputs range: (0,1)
Issues:

Saturates and kills gradients.
Not zero-centered.
Computationally expensive.



Tanh Function
\tanh(x) = \frac{e^x - e^{-x}}{e^x &#43; e^{-x}}

Outputs range: (-1,1)
Zero-centered but still prone to gradient saturation.

ReLU (Rectified Linear Unit)
f(x) = \max(0, x)

Does not saturate in the positive region.
Computationally efficient.
Drawbacks:

Can lead to &ldquo;dead neurons&rdquo; (never activating).
Not zero-centered.



Leaky ReLU
f(x) = \max(\alpha x, x), \quad \alpha \approx 0.01

Addresses dead neuron issue by allowing small negative values.

ELU (Exponential Linear Unit)
f(x) = \begin{cases}
x, &amp; x &gt; 0 \\
\alpha (e^x - 1), &amp; x \leq 0
\end{cases}

Zero-centered.
More robust than ReLU in noise-prone settings.

Maxout
f(x) = \max(w_1^T x &#43; b_1, w_2^T x &#43; b_2)

Generalizes ReLU and Leaky ReLU.
Doubles parameters per neuron.

Practical Choice

Use ReLU as the default.
Try Leaky ReLU, Maxout, or ELU.
Avoid Sigmoid.

Data Preprocessing
Zero Mean
X \leftarrow X - \text{mean}(X, \text{axis}=0)
Normalization
X \leftarrow \frac{X}{\text{std}(X, \text{axis}=0)}
PCA &amp; Whitening

PCA decorrelates data.
Whitening scales dimensions by eigenvalues.

Key Rule

Compute statistics only on training data.
Apply transformation to test/validation data.

Weight Initialization

Proper initialization prevents gradient vanishing or explosion.

w = \frac{\text{randn}(n)}{\sqrt{n}}

For ReLU:

w = \frac{\text{randn}(n)}{\sqrt{n/2}}
Batch Normalization (BN)

Reduces internal covariate shift.
BN normalizes activations:

\hat{x} = \frac{x - \mu}{\sigma}

Applied before the activation function.
At test time, uses fixed mean/std from training.

Regularization
L2 Regularization (Weight Decay)
R(f) = \lambda ||w||^2
L1 Regularization
R(f) = \lambda ||w||

Leads to sparse weights.

Elastic Net

Combination of L1 and L2.

Dropout

Randomly drops neurons during training.
Test-time: Scale weights by dropout probability ( p ).

Hyperparameter Optimization

Random Search &gt; Grid Search.
Bayesian Optimization speeds up the process.
Common Hyperparameters:

Learning rate.
Batch size.
Number of layers.
Dropout rate.



Loss Functions
Classification

Softmax with Cross-Entropy Loss:

L = - \sum_{i} y_i \log(\hat{y}_i)

Hinge Loss (for SVMs):

L = \sum_{i} \max(0, 1 - y_i \hat{y}_i)
Regression

L2 Loss:

L = \sum_i (y_i - \hat{y}_i)^2

L1 Loss:

L = \sum_i |y_i - \hat{y}_i|
Parameter Update

After computing gradients via backpropagation, parameters are updated using:

Vanilla SGD
w \leftarrow w - \eta \nabla w
Momentum Update
v \leftarrow \beta v - \eta \nabla w
w \leftarrow w &#43; v
Nesterov Accelerated Gradient
v \leftarrow \beta v - \eta \nabla w&#39;
w&#39; \leftarrow w &#43; \beta v
Final Notes

Choose activation functions wisely (ReLU recommended).
Use normalization and proper weight initialization.
Apply regularization techniques (L2, Dropout).
Optimize hyperparameters systematically (Bayesian search preferred).
Understand loss functions based on classification vs. regression.
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Midterm_summaries",
      "item": "https://felipecordero.github.io/rnn_class/midterm_summary/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Chapter 1",
      "item": "https://felipecordero.github.io/rnn_class/midterm_summary/chapter-1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Chapter 1",
  "name": "Chapter 1",
  "description": " Neural Network Fundamentals Multi-layered Neural Network Structure: Input: 10 numbers Output: 4 decisions or predictions Backpropagation: Used to calculate the error in predictions and adjust the network. Sensitivity in Neural Networks Sensitivity measures how much the error at the output ((\\Delta E)) changes due to small input variations. Sensitivity is propagated backward using: $$ \\delta_{p1} = \\frac{\\partial E}{\\partial p1} $$ Error computation block: \\{2(o1 - t1), 2(o2 - t2), 2(o3 - t3), 2(o4 - t4)\\} Activation Functions Sigmoid Function \\sigma(x) = \\frac{1}{1 + e^{-x}} Outputs range: (0,1) Issues: Saturates and kills gradients. Not zero-centered. Computationally expensive. Tanh Function \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} Outputs range: (-1,1) Zero-centered but still prone to gradient saturation. ReLU (Rectified Linear Unit) f(x) = \\max(0, x) Does not saturate in the positive region. Computationally efficient. Drawbacks: Can lead to \u0026ldquo;dead neurons\u0026rdquo; (never activating). Not zero-centered. Leaky ReLU f(x) = \\max(\\alpha x, x), \\quad \\alpha \\approx 0.01 Addresses dead neuron issue by allowing small negative values. ELU (Exponential Linear Unit) f(x) = \\begin{cases} x, \u0026amp; x \u0026gt; 0 \\\\ \\alpha (e^x - 1), \u0026amp; x \\leq 0 \\end{cases} Zero-centered. More robust than ReLU in noise-prone settings. Maxout f(x) = \\max(w_1^T x + b_1, w_2^T x + b_2) Generalizes ReLU and Leaky ReLU. Doubles parameters per neuron. Practical Choice Use ReLU as the default. Try Leaky ReLU, Maxout, or ELU. Avoid Sigmoid. Data Preprocessing Zero Mean X \\leftarrow X - \\text{mean}(X, \\text{axis}=0) Normalization X \\leftarrow \\frac{X}{\\text{std}(X, \\text{axis}=0)} PCA \u0026amp; Whitening PCA decorrelates data. Whitening scales dimensions by eigenvalues. Key Rule Compute statistics only on training data. Apply transformation to test/validation data. Weight Initialization Proper initialization prevents gradient vanishing or explosion. w = \\frac{\\text{randn}(n)}{\\sqrt{n}} For ReLU: w = \\frac{\\text{randn}(n)}{\\sqrt{n/2}} Batch Normalization (BN) Reduces internal covariate shift. BN normalizes activations: \\hat{x} = \\frac{x - \\mu}{\\sigma} Applied before the activation function. At test time, uses fixed mean/std from training. Regularization L2 Regularization (Weight Decay) R(f) = \\lambda ||w||^2 L1 Regularization R(f) = \\lambda ||w|| Leads to sparse weights. Elastic Net Combination of L1 and L2. Dropout Randomly drops neurons during training. Test-time: Scale weights by dropout probability ( p ). Hyperparameter Optimization Random Search \u0026gt; Grid Search. Bayesian Optimization speeds up the process. Common Hyperparameters: Learning rate. Batch size. Number of layers. Dropout rate. Loss Functions Classification Softmax with Cross-Entropy Loss: L = - \\sum_{i} y_i \\log(\\hat{y}_i) Hinge Loss (for SVMs): L = \\sum_{i} \\max(0, 1 - y_i \\hat{y}_i) Regression L2 Loss: L = \\sum_i (y_i - \\hat{y}_i)^2 L1 Loss: L = \\sum_i |y_i - \\hat{y}_i| Parameter Update After computing gradients via backpropagation, parameters are updated using: Vanilla SGD w \\leftarrow w - \\eta \\nabla w Momentum Update v \\leftarrow \\beta v - \\eta \\nabla w w \\leftarrow w + v Nesterov Accelerated Gradient v \\leftarrow \\beta v - \\eta \\nabla w\u0026#39; w\u0026#39; \\leftarrow w + \\beta v Final Notes Choose activation functions wisely (ReLU recommended). Use normalization and proper weight initialization. Apply regularization techniques (L2, Dropout). Optimize hyperparameters systematically (Bayesian search preferred). Understand loss functions based on classification vs. regression. ",
  "keywords": [
    
  ],
  "articleBody": " Neural Network Fundamentals Multi-layered Neural Network Structure: Input: 10 numbers Output: 4 decisions or predictions Backpropagation: Used to calculate the error in predictions and adjust the network. Sensitivity in Neural Networks Sensitivity measures how much the error at the output ((\\Delta E)) changes due to small input variations. Sensitivity is propagated backward using: $$ \\delta_{p1} = \\frac{\\partial E}{\\partial p1} $$ Error computation block: \\{2(o1 - t1), 2(o2 - t2), 2(o3 - t3), 2(o4 - t4)\\} Activation Functions Sigmoid Function \\sigma(x) = \\frac{1}{1 + e^{-x}} Outputs range: (0,1) Issues: Saturates and kills gradients. Not zero-centered. Computationally expensive. Tanh Function \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} Outputs range: (-1,1) Zero-centered but still prone to gradient saturation. ReLU (Rectified Linear Unit) f(x) = \\max(0, x) Does not saturate in the positive region. Computationally efficient. Drawbacks: Can lead to “dead neurons” (never activating). Not zero-centered. Leaky ReLU f(x) = \\max(\\alpha x, x), \\quad \\alpha \\approx 0.01 Addresses dead neuron issue by allowing small negative values. ELU (Exponential Linear Unit) f(x) = \\begin{cases} x, \u0026 x \u003e 0 \\\\ \\alpha (e^x - 1), \u0026 x \\leq 0 \\end{cases} Zero-centered. More robust than ReLU in noise-prone settings. Maxout f(x) = \\max(w_1^T x + b_1, w_2^T x + b_2) Generalizes ReLU and Leaky ReLU. Doubles parameters per neuron. Practical Choice Use ReLU as the default. Try Leaky ReLU, Maxout, or ELU. Avoid Sigmoid. Data Preprocessing Zero Mean X \\leftarrow X - \\text{mean}(X, \\text{axis}=0) Normalization X \\leftarrow \\frac{X}{\\text{std}(X, \\text{axis}=0)} PCA \u0026 Whitening PCA decorrelates data. Whitening scales dimensions by eigenvalues. Key Rule Compute statistics only on training data. Apply transformation to test/validation data. Weight Initialization Proper initialization prevents gradient vanishing or explosion. w = \\frac{\\text{randn}(n)}{\\sqrt{n}} For ReLU: w = \\frac{\\text{randn}(n)}{\\sqrt{n/2}} Batch Normalization (BN) Reduces internal covariate shift. BN normalizes activations: \\hat{x} = \\frac{x - \\mu}{\\sigma} Applied before the activation function. At test time, uses fixed mean/std from training. Regularization L2 Regularization (Weight Decay) R(f) = \\lambda ||w||^2 L1 Regularization R(f) = \\lambda ||w|| Leads to sparse weights. Elastic Net Combination of L1 and L2. Dropout Randomly drops neurons during training. Test-time: Scale weights by dropout probability ( p ). Hyperparameter Optimization Random Search \u003e Grid Search. Bayesian Optimization speeds up the process. Common Hyperparameters: Learning rate. Batch size. Number of layers. Dropout rate. Loss Functions Classification Softmax with Cross-Entropy Loss: L = - \\sum_{i} y_i \\log(\\hat{y}_i) Hinge Loss (for SVMs): L = \\sum_{i} \\max(0, 1 - y_i \\hat{y}_i) Regression L2 Loss: L = \\sum_i (y_i - \\hat{y}_i)^2 L1 Loss: L = \\sum_i |y_i - \\hat{y}_i| Parameter Update After computing gradients via backpropagation, parameters are updated using: Vanilla SGD w \\leftarrow w - \\eta \\nabla w Momentum Update v \\leftarrow \\beta v - \\eta \\nabla w w \\leftarrow w + v Nesterov Accelerated Gradient v \\leftarrow \\beta v - \\eta \\nabla w' w' \\leftarrow w + \\beta v Final Notes Choose activation functions wisely (ReLU recommended). Use normalization and proper weight initialization. Apply regularization techniques (L2, Dropout). Optimize hyperparameters systematically (Bayesian search preferred). Understand loss functions based on classification vs. regression. ",
  "wordCount" : "503",
  "inLanguage": "en",
  "datePublished": "2025-03-10T12:39:55-04:00",
  "dateModified": "2025-03-10T12:39:55-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://felipecordero.github.io/rnn_class/midterm_summary/chapter-1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "RNN Class",
    "logo": {
      "@type": "ImageObject",
      "url": "https://felipecordero.github.io/rnn_class/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://felipecordero.github.io/rnn_class/" accesskey="h" title="RNN Class (Alt + H)">RNN Class</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-1/" title="Chapter 1">
                    <span class="active">Chapter 1</span>
                </a>
            </li>
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-2/" title="Chapter 2">
                    <span>Chapter 2</span>
                </a>
            </li>
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-3/" title="Chapter 3">
                    <span>Chapter 3</span>
                </a>
            </li>
            <li>
                <a href="https://felipecordero.github.io/rnn_class/midterm_summary/chapter-4/" title="Chapter 4">
                    <span>Chapter 4</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Chapter 1
    </h1>
    <div class="post-meta"><span title='2025-03-10 12:39:55 -0400 EDT'>March 10, 2025</span>

</div>
  </header> 
  <div class="post-content"><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    document.querySelectorAll("pre code.language-math").forEach((block) => {
        const mathContent = block.textContent.trim();
        const displayMode = mathContent.includes("\n") ? "\\[" + mathContent + "\\]" : "\\(" + mathContent + "\\)";
        const wrapper = document.createElement("div");
        wrapper.innerHTML = displayMode;
        block.parentNode.replaceWith(wrapper);
    });

    if (window.MathJax) {
        MathJax.typesetPromise();
    }
});
</script>

<h1 id="neural-network-fundamentals">Neural Network Fundamentals<a hidden class="anchor" aria-hidden="true" href="#neural-network-fundamentals">#</a></h1>
<h2 id="multi-layered-neural-network">Multi-layered Neural Network<a hidden class="anchor" aria-hidden="true" href="#multi-layered-neural-network">#</a></h2>
<ul>
<li><strong>Structure</strong>:
<ul>
<li>Input: 10 numbers</li>
<li>Output: 4 decisions or predictions</li>
</ul>
</li>
<li><strong>Backpropagation</strong>:
<ul>
<li>Used to calculate the error in predictions and adjust the network.</li>
</ul>
</li>
</ul>
<h2 id="sensitivity-in-neural-networks">Sensitivity in Neural Networks<a hidden class="anchor" aria-hidden="true" href="#sensitivity-in-neural-networks">#</a></h2>
<ul>
<li>Sensitivity measures how much the error at the output ((\Delta E)) changes due to small input variations.</li>
<li>Sensitivity is propagated backward using:
$$
\delta_{p1} = \frac{\partial E}{\partial p1}
$$</li>
<li>Error computation block:
<pre tabindex="0"><code class="language-math" data-lang="math">\{2(o1 - t1), 2(o2 - t2), 2(o3 - t3), 2(o4 - t4)\}
</code></pre></li>
</ul>
<h2 id="activation-functions">Activation Functions<a hidden class="anchor" aria-hidden="true" href="#activation-functions">#</a></h2>
<h3 id="sigmoid-function">Sigmoid Function<a hidden class="anchor" aria-hidden="true" href="#sigmoid-function">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">\sigma(x) = \frac{1}{1 + e^{-x}}
</code></pre><ul>
<li>Outputs range: (0,1)</li>
<li><strong>Issues</strong>:
<ul>
<li>Saturates and kills gradients.</li>
<li>Not zero-centered.</li>
<li>Computationally expensive.</li>
</ul>
</li>
</ul>
<h3 id="tanh-function">Tanh Function<a hidden class="anchor" aria-hidden="true" href="#tanh-function">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
</code></pre><ul>
<li>Outputs range: (-1,1)</li>
<li>Zero-centered but still prone to gradient saturation.</li>
</ul>
<h3 id="relu-rectified-linear-unit">ReLU (Rectified Linear Unit)<a hidden class="anchor" aria-hidden="true" href="#relu-rectified-linear-unit">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">f(x) = \max(0, x)
</code></pre><ul>
<li>Does not saturate in the positive region.</li>
<li>Computationally efficient.</li>
<li><strong>Drawbacks</strong>:
<ul>
<li>Can lead to &ldquo;dead neurons&rdquo; (never activating).</li>
<li>Not zero-centered.</li>
</ul>
</li>
</ul>
<h3 id="leaky-relu">Leaky ReLU<a hidden class="anchor" aria-hidden="true" href="#leaky-relu">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">f(x) = \max(\alpha x, x), \quad \alpha \approx 0.01
</code></pre><ul>
<li>Addresses dead neuron issue by allowing small negative values.</li>
</ul>
<h3 id="elu-exponential-linear-unit">ELU (Exponential Linear Unit)<a hidden class="anchor" aria-hidden="true" href="#elu-exponential-linear-unit">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">f(x) = \begin{cases}
x, &amp; x &gt; 0 \\
\alpha (e^x - 1), &amp; x \leq 0
\end{cases}
</code></pre><ul>
<li>Zero-centered.</li>
<li>More robust than ReLU in noise-prone settings.</li>
</ul>
<h3 id="maxout">Maxout<a hidden class="anchor" aria-hidden="true" href="#maxout">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">f(x) = \max(w_1^T x + b_1, w_2^T x + b_2)
</code></pre><ul>
<li>Generalizes ReLU and Leaky ReLU.</li>
<li>Doubles parameters per neuron.</li>
</ul>
<h3 id="practical-choice">Practical Choice<a hidden class="anchor" aria-hidden="true" href="#practical-choice">#</a></h3>
<ul>
<li>Use <strong>ReLU</strong> as the default.</li>
<li>Try <strong>Leaky ReLU, Maxout, or ELU</strong>.</li>
<li>Avoid <strong>Sigmoid</strong>.</li>
</ul>
<h2 id="data-preprocessing">Data Preprocessing<a hidden class="anchor" aria-hidden="true" href="#data-preprocessing">#</a></h2>
<h3 id="zero-mean">Zero Mean<a hidden class="anchor" aria-hidden="true" href="#zero-mean">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">X \leftarrow X - \text{mean}(X, \text{axis}=0)
</code></pre><h3 id="normalization">Normalization<a hidden class="anchor" aria-hidden="true" href="#normalization">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">X \leftarrow \frac{X}{\text{std}(X, \text{axis}=0)}
</code></pre><h3 id="pca--whitening">PCA &amp; Whitening<a hidden class="anchor" aria-hidden="true" href="#pca--whitening">#</a></h3>
<ul>
<li>PCA decorrelates data.</li>
<li>Whitening scales dimensions by eigenvalues.</li>
</ul>
<h3 id="key-rule">Key Rule<a hidden class="anchor" aria-hidden="true" href="#key-rule">#</a></h3>
<ul>
<li>Compute statistics <strong>only on training data</strong>.</li>
<li>Apply transformation to test/validation data.</li>
</ul>
<h2 id="weight-initialization">Weight Initialization<a hidden class="anchor" aria-hidden="true" href="#weight-initialization">#</a></h2>
<ul>
<li>Proper initialization prevents gradient vanishing or explosion.</li>
</ul>
<pre tabindex="0"><code class="language-math" data-lang="math">w = \frac{\text{randn}(n)}{\sqrt{n}}
</code></pre><ul>
<li>For ReLU:</li>
</ul>
<pre tabindex="0"><code class="language-math" data-lang="math">w = \frac{\text{randn}(n)}{\sqrt{n/2}}
</code></pre><h2 id="batch-normalization-bn">Batch Normalization (BN)<a hidden class="anchor" aria-hidden="true" href="#batch-normalization-bn">#</a></h2>
<ul>
<li><strong>Reduces internal covariate shift</strong>.</li>
<li>BN normalizes activations:</li>
</ul>
<pre tabindex="0"><code class="language-math" data-lang="math">\hat{x} = \frac{x - \mu}{\sigma}
</code></pre><ul>
<li>Applied before the activation function.</li>
<li>At test time, uses fixed mean/std from training.</li>
</ul>
<h2 id="regularization">Regularization<a hidden class="anchor" aria-hidden="true" href="#regularization">#</a></h2>
<h3 id="l2-regularization-weight-decay">L2 Regularization (Weight Decay)<a hidden class="anchor" aria-hidden="true" href="#l2-regularization-weight-decay">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">R(f) = \lambda ||w||^2
</code></pre><h3 id="l1-regularization">L1 Regularization<a hidden class="anchor" aria-hidden="true" href="#l1-regularization">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">R(f) = \lambda ||w||
</code></pre><ul>
<li>Leads to sparse weights.</li>
</ul>
<h3 id="elastic-net">Elastic Net<a hidden class="anchor" aria-hidden="true" href="#elastic-net">#</a></h3>
<ul>
<li>Combination of L1 and L2.</li>
</ul>
<h3 id="dropout">Dropout<a hidden class="anchor" aria-hidden="true" href="#dropout">#</a></h3>
<ul>
<li>Randomly drops neurons during training.</li>
<li><strong>Test-time</strong>: Scale weights by dropout probability ( p ).</li>
</ul>
<h2 id="hyperparameter-optimization">Hyperparameter Optimization<a hidden class="anchor" aria-hidden="true" href="#hyperparameter-optimization">#</a></h2>
<ul>
<li><strong>Random Search</strong> &gt; Grid Search.</li>
<li>Bayesian Optimization speeds up the process.</li>
<li><strong>Common Hyperparameters</strong>:
<ul>
<li>Learning rate.</li>
<li>Batch size.</li>
<li>Number of layers.</li>
<li>Dropout rate.</li>
</ul>
</li>
</ul>
<h2 id="loss-functions">Loss Functions<a hidden class="anchor" aria-hidden="true" href="#loss-functions">#</a></h2>
<h3 id="classification">Classification<a hidden class="anchor" aria-hidden="true" href="#classification">#</a></h3>
<ul>
<li><strong>Softmax with Cross-Entropy Loss</strong>:</li>
</ul>
<pre tabindex="0"><code class="language-math" data-lang="math">L = - \sum_{i} y_i \log(\hat{y}_i)
</code></pre><ul>
<li><strong>Hinge Loss</strong> (for SVMs):</li>
</ul>
<pre tabindex="0"><code class="language-math" data-lang="math">L = \sum_{i} \max(0, 1 - y_i \hat{y}_i)
</code></pre><h3 id="regression">Regression<a hidden class="anchor" aria-hidden="true" href="#regression">#</a></h3>
<ul>
<li><strong>L2 Loss</strong>:</li>
</ul>
<pre tabindex="0"><code class="language-math" data-lang="math">L = \sum_i (y_i - \hat{y}_i)^2
</code></pre><ul>
<li><strong>L1 Loss</strong>:</li>
</ul>
<pre tabindex="0"><code class="language-math" data-lang="math">L = \sum_i |y_i - \hat{y}_i|
</code></pre><h2 id="parameter-update">Parameter Update<a hidden class="anchor" aria-hidden="true" href="#parameter-update">#</a></h2>
<ul>
<li>After computing gradients via <strong>backpropagation</strong>, parameters are updated using:</li>
</ul>
<h3 id="vanilla-sgd">Vanilla SGD<a hidden class="anchor" aria-hidden="true" href="#vanilla-sgd">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">w \leftarrow w - \eta \nabla w
</code></pre><h3 id="momentum-update">Momentum Update<a hidden class="anchor" aria-hidden="true" href="#momentum-update">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">v \leftarrow \beta v - \eta \nabla w
</code></pre><pre tabindex="0"><code class="language-math" data-lang="math">w \leftarrow w + v
</code></pre><h3 id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient<a hidden class="anchor" aria-hidden="true" href="#nesterov-accelerated-gradient">#</a></h3>
<pre tabindex="0"><code class="language-math" data-lang="math">v \leftarrow \beta v - \eta \nabla w&#39;
</code></pre><pre tabindex="0"><code class="language-math" data-lang="math">w&#39; \leftarrow w + \beta v
</code></pre><h2 id="final-notes">Final Notes<a hidden class="anchor" aria-hidden="true" href="#final-notes">#</a></h2>
<ul>
<li><strong>Choose activation functions wisely</strong> (ReLU recommended).</li>
<li><strong>Use normalization and proper weight initialization</strong>.</li>
<li><strong>Apply regularization techniques</strong> (L2, Dropout).</li>
<li><strong>Optimize hyperparameters systematically</strong> (Bayesian search preferred).</li>
<li><strong>Understand loss functions</strong> based on classification vs. regression.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://felipecordero.github.io/rnn_class/">RNN Class</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
